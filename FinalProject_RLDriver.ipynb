{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73x8Vm6jblY-"
   },
   "source": [
    "# [RL Autonomous Driver](https://github.com/tonyroumi/COGS188_RLAutonomousDriver)\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Ivan Guo\n",
    "- Ruiqi(Ricky) Zhu\n",
    "- Anthony Roumi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeygYhfgblY_"
   },
   "source": [
    "# Abstract\n",
    "This project implements a reinforcement learning approach to autonomous driving using the CARLA simulator. The goal was to develop an agent capable of driving in urban environments while adhering to traffic rules and avoiding collisions. The model uses an Advantage Actor-Crtic (A3C) architecture that processes RGB camera images as input states and outputs discrete vehicle control commands. The agent learns through reward functions that incetivize progress towards the destination while penalizing unsafe behaviors like speeding, collisions, traffic light violations, and lane-keeping. Training data is collected through interaction with the CARLA environment, where the agent explores using an epislon-greedy strategy. This solution utilizes convolutional neural networks for visual processing with prioritized experience replay to enhance sample efficiency. The primary learning mechanism relies on end-to-end training from pixels to actions. Evaluation metrics track collision rates, success rates, and cumulative rewards across training episodes. Given the challenging nature of the task, the agent struggles to adapt to driving scenarios without adequate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPEz4FoWblZA"
   },
   "source": [
    "# Background\n",
    "\n",
    "In recent years, autonomous driving has made great strides in the field of Artificial Intelligence (AI), particularly in perception and decision-making systems. The primary objective of the community is to enable vehicles to make informed decisions and safely navigate complex road conditions without human intervention. To achieve this vehicles must be capable of developing a comprehensive understanding of the environment and given the ability to make high-fidelity predictions about the future evolution of the driving scene.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) One of the major challenges lies in the accurate detection and prediction of adverse events, such as the appearance and recognition of occluded vehicles and sudden unexpected behaviors of the world environment.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)\n",
    "\n",
    "To address these challenges, researchers have developed end-to-end algorithm frameworks that allow autonomous vehicles to dynamically adapt to complex and unpredictable environments. Many of these methods focus on the behavior of experts combined with learning features of the driving scene to make predictions, plan trajectories, and take actions. Broadly speaking, these approaches fall into two main categories: trajectory-based controllers and direct control prediction. <a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)\n",
    "\n",
    "Trajectory-based methods leverage intermediate representations—such as vector space representations or bird’s-eye view maps—to decouple perception from control. By first constructing a structured understanding of the driving scene, these methods enable safer and more interpretable decision-making. This modular approach facilitates the application of traditional control techniques, such as Model Predictive Control (MPC) or Rule Based methods, and allows for easier integration of safety checks, as errors can be localized within a specific module. [<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3)<a name=\"cite_ref-7\"></a>,[<sup>7</sup>](#cite_note-7)] Furthermore, trajectory-based systems can incorporate probabilistic forecasting models to anticipate the motion of other agents in the scene, reducing the risk of unexpected interactions.<a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4)\n",
    "\n",
    "In contrast, direct control prediction approaches bypass the need for explicit intermediate representations by mapping raw sensor inputs directly to low-level commands—such as steering, acceleration, and braking. This end-to-end paradigm enables joint optimization of the entire pipeline, which can yield a more simple and efficient system. Researchers are also exploring hybrid frameworks that combine the strengths of both methods: using intermediate representations to provide structured safety checks while also leveraging direct control predictions for faster, more adaptable decision-making in unpredictable real-world scenarios.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2)\n",
    "\n",
    "Studying autonomous vehicle capabilities is important to improve road safety, reduce traffic congestion, and enhance mobility. However, real-world testing of autonomous vehicles presents significant challenges, including high costs, safety risks in uncontrolled urban environments, and the need for policy adaptations by local governments<a name=\"cite_ref-5\"></a>[<sup>5</sup>](#cite_note-5).\n",
    "\n",
    "Simulation platforms, like the CARLA simulator, offer a safe, repeatable, and scalable playground where researchers can evaluate autonomous driving algorithms across a diverse array of scenarios.<a name=\"cite_ref-6\"></a>[<sup>6</sup>](#cite_note-6) By recreating adverse weather conditions, complex traffic patterns, and rare corner cases within a controlled virtual environment, simulation enables rapid prototyping and iterative refinement of both trajectory-based and end-to-end control methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiVxH96jblZA"
   },
   "source": [
    "# Problem Statement\n",
    "We aim to create an AI agent that can autonomously drive in a realistic simulation environment (CARLA) by processing sensor data and making safe control decisions. The problem is defined as an MDP with observable states (camera images), discrete actions (vehicle control commands), and rewards that penalize collisions, red-light violations, and idleness while rewarding progress toward a destination. The task is quantifiable (via cumulative reward and collision counts), measurable (through defined evaluation metrics), and replicable (using standardized simulation scenarios).\n",
    "\n",
    "The challenge is to learn an autonomous driving policy that accurately predicts control actions at each time step(throttle, brake, and steer). Formally, given a state x — an image from a single camera — the goal is to learn a policy $\\pi\\theta$ that outputs control actions $a = (a_t, a_{t+1}, ..., a_{t+K})$ at each time step.\n",
    "\n",
    "The problem is defined as follows:\n",
    "\n",
    "- Quantifiable: The system’s performance can be expressed in terms of measurable metrics such as collision rate, travel time, and traffic regulation penalties. Improvements can be quantified against existing models on [CARLA Leaderboard Benchmark](https://paperswithcode.com/sota/autonomous-driving-on-carla-leaderboard)\n",
    "- Measurable: Success will be measured by the system’s ability to complete driving missions without collisions, while optimizing efficiency/travel time and obeying traffic regulations.\n",
    "- Replicable: The use of a standardized simulation environment (CARLA) ensures that experimental conditions, sensor inputs, and traffic scenarios can be consistently reproduced and tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-Xim9vtblZA"
   },
   "source": [
    "# Data Collection\n",
    "The training data is collected in real-time through direct interaction with the CARLA simulator. The agent generates its own data by exploring the environment and learning from experience, following the reinforcement learning (RL) paradigm.\n",
    "\n",
    "## Sensor Inputs\n",
    "The primary data sources include:\n",
    "- **RGB Camera Sensor:** Mounted at the front of the vehicle, capturing images at **10 FPS** with a resolution of **384x256 pixels** and a **135-degree field of view (FOV)**. \n",
    "- **Collision Sensor:** Detects collisions with other actors (vehicles, pedestrians, static objects) and records events to compute penalties based on the collision type.\n",
    "- **LiDAR Sensor:** Generates **point cloud data** representing distances and intensities. \n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"assets/test_camera_image.jpg\" width=\"400\" height=\"300\" style=\"display:inline-block; margin: 0 10px;\"> \n",
    "  <img src=\"assets/debug_lidar.png\" width=\"400\" height=\"300\" style=\"display:inline-block; margin: 0 10px;\">\n",
    "  <p style=\"font-size: 0.9em; font-style: italic;\">Figure Left: Sample RGB camera input (384x256 pixels) used as the primary state representation for the agent.</p>\n",
    "  <p style=\"font-size: 0.9em; font-style: italic;\">Figure Right: Visualization of LiDAR point cloud data providing depth information.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Data is collected in rollouts of up to 20 timesteps, where each rollout consists of state-action-reward-value tuples. The camera sensor captures images at fixed intervals (**0.1 seconds**), which are used as input for the neural network to predict action probabilities and state values.\n",
    "\n",
    "## Image Processing\n",
    "The raw camera images undergo a preprocessing pipeline:\n",
    "1. Conversion from a raw buffer to a NumPy array.\n",
    "2. Removal of the alpha channel, keeping only RGB values.\n",
    "3. Transformation into a PyTorch tensor and transposition to set channels as the first dimension.\n",
    "4. **Normalization:** Pixel values are scaled to [0,1].\n",
    "5. **HSV Color Thresholding:** To detect red lights, the image is converted from BGR to HSV color space, and two red masks are applied to cover both ends of the hue spectrum. The central region is analyzed, and a red light is detected if red pixel concentration exceeds 20%.\n",
    "\n",
    "### Dataset Characteristics\n",
    "The dataset is generated as the agent interacts with the CARLA environment. A prioritized experience replay (PER) buffer enhances sample efficiency by storing transitions with higher temporal-difference (TD) errors, see the [Supplementary Materials](#Supplementary-Materials) section.. \n",
    "\n",
    "### State Representation:\n",
    "The primary input to the neural network consists of processed camera images, providing a visual representation of the environment for decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H-EWpuAblZB"
   },
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The solution implements a deep reinforcement learning approach using an Advantage Actor-Critic (A3C) architecture. This approach enables end-to-end learning from raw camera inputs to vehicle control commands through direct interaction with the environment.\n",
    "\n",
    "The model consists of an Actor (Policy) Network which determines which actions to take in a given state; and a Critic (Value) Network which estimates the expected return from each state. These components share convolutional layers to process visual inputs while maintaining separate output heads for policy and value estimation. For detailed network architecture, see the [Supplementary Materials](#Supplementary-Materials) section.\n",
    "\n",
    "The network processes 384×256 RGB images and outputs logits for 6 discrete actions (steer left, steer right, throttle, brake, maintain speed, reverse) along with a value estimate of the current state.\n",
    "\n",
    "Our implementation uses a synchronous version of the Advantage Actor-Critic  algorithm with the following components:\n",
    "1. Rollout Collection: The agent interacts with the environment for 20 timesteps, collecting state-action-reward-value tuples.\n",
    "Multi-step Returns: For each state in the rollout, a bootstrapped return is calculated using:\n",
    "   $$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^n V(s_{t+n})$$\n",
    "  \n",
    "  where $\\gamma =0.9$ .\n",
    "\n",
    "2. Advantage Calculation: The advantage is computed as the difference between the return and the value estimate:\n",
    "   $$A(s_t, a_t) = R_t - V(s_t)$$\n",
    "\n",
    "3. Loss Function: The network is updated by minimizing:\n",
    "   $$L = L_{\\text{policy}} + \\beta \\cdot L_{\\text{value}} - \\alpha \\cdot L_{\\text{entropy}}$$\n",
    "\n",
    "where:\n",
    "$$L_{\\text{policy}} = -\\log(\\pi(a_t|s_t)) \\cdot A(s_t, a_t)$$\n",
    "$$L_{\\text{value}} = (V(s_t) - R_t)^2$$\n",
    "$$L_{\\text{entropy}} = -\\sum \\pi(a|s_t) \\cdot \\log(\\pi(a|s_t))$$\n",
    "$$\\beta = 0.5 \\text{ (value loss coefficient)}$$\n",
    "$$\\alpha = 0.05 \\text{ (entropy coefficient)}$$\n",
    "\n",
    "We utilize a prioritized experience replay where transitions are stored with priorities based on their TD errors, allowing more informative experiences to be sampled more frequently during off-policy updates.\n",
    "\n",
    "### Reward Function Design\n",
    "The reward function integrates multiple aspects of driving behavior:\n",
    "- **Speed Compliance:** Rewards are given for maintaining a safe speed.\n",
    "- **Collision Penalties:** Negative rewards are applied for collisions, with severity based on the type of collision.\n",
    "- **Idling and Reversing Penalties:** Penalizes excessive idling or reversing.\n",
    "- **Progress Toward Destination:** Rewards are assigned based on the decrease in distance to the target, with a large reward for reaching the goal.\n",
    "- **Traffic and Lane Compliance:** Negative rewards are given for violations such as running red lights or deviating from the lane.\n",
    "\n",
    "### Action Selection:\n",
    "The Actor-Critic network outputs a stochastic policy, and actions are chosen using epsilon-greedy exploration. Possible actions include:\n",
    "- Steering (Lateral)\n",
    "- Throttling (Longitudinal)\n",
    "- Braking (Longitudinal)\n",
    "- Reversing (Longitudinal)\n",
    "\n",
    "### Learning Through Rewards:\n",
    "The agent refines its policy to optimize the return of rewards at the current timestep. Training updates use temporal difference learning to optimize the agent's behavior over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8mQ7F5sblZB"
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We evaluate our reinforcement learning agent using several metrics that capture different aspects of autonomous driving performance:\n",
    "\n",
    "## 1. Success Rate\n",
    "The primary metric is the success rate, defined as the percentage of episodes where the agent successfully reaches the destination without collisions or traffic violations:\n",
    "\n",
    "$$\\text{Success Rate} = \\frac{\\text{Number of successful episodes}}{\\text{Total number of episodes}} \\times 100\\%$$\n",
    "\n",
    "A successful episode requires the agent to navigate from start to destination while maintaining safety and following traffic rules.\n",
    "\n",
    "## 2. Collision Rate\n",
    "This metric measures the frequency of collisions per episode:\n",
    "\n",
    "$$\\text{Collision Rate} = \\frac{\\text{Total number of collisions}}{\\text{Total number of episodes}}$$\n",
    "\n",
    "Lower values indicate safer driving behavior. We further categorize collisions by type (vehicle, pedestrian, static object) to provide more detailed analysis.\n",
    "\n",
    "## 3. Cumulative Reward\n",
    "The total reward accumulated over an episode reflects the overall quality of the driving policy:\n",
    "\n",
    "$$\\text{Cumulative Reward} = \\sum_{t=0}^{T} r_t$$\n",
    "\n",
    "where $r_t$ is the reward at timestep $t$ and $T$ is the episode length. Higher values indicate better performance across all reward components (progress, safety, rule compliance).\n",
    "\n",
    "## 4. Average Speed\n",
    "This metric evaluates the efficiency of the driving policy:\n",
    "\n",
    "$$\\text{Average Speed} = \\frac{1}{T} \\sum_{t=0}^{T} v_t$$\n",
    "\n",
    "where $v_t$ is the vehicle speed at timestep $t$. This helps assess whether the agent maintains appropriate speeds without excessive stopping or speeding.\n",
    "\n",
    "## 5. Traffic Rule Violation Rate\n",
    "This measures the frequency of traffic violations per episode:\n",
    "\n",
    "$$\\text{Violation Rate} = \\frac{\\text{Number of traffic violations}}{\\text{Total number of episodes}}$$\n",
    "\n",
    "Violations include running red lights and lane departures, with lower values indicating better rule compliance.\n",
    "\n",
    "These metrics collectively provide a comprehensive evaluation of the agent's performance, balancing safety, efficiency, and goal achievement in autonomous driving tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V27AAi7SblZB"
   },
   "source": [
    "# Results\n",
    "\n",
    "Our analysis focuses on the performance of the A3C reinforcement learning agent in the CARLA simulator environment. We present results from training over XXX episodes.\n",
    "\n",
    "## Training Performance\n",
    "\n",
    "The primary challenge in training our agent became evident through the cumulative reward trajectory. As shown in Figure 1, the agent struggled to achieve consistent positive rewards.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"assets/reward_graph_episode_6000.png\" width=\"500\" alt=\"Training Rewards\">\n",
    "    <p><em>Figure 1: Cumulative reward at episode 6000 during training.</em></p>\n",
    "</div>\n",
    "\n",
    "The reward volatility reflects the complexity of the autonomous driving task when learning directly from pixels. As shown in Figure 1, the rewards spike frequently and struggle to stabilize, indicating the agent's difficulty in converging on a consistent driving strategy. Despite training for 6000 episodes, the agent was unable to develop an optimal policy; likely due to constraints in resources and training time. The high-dimensional nature of learning from raw camera inputs requires substantially more training iterations and more complex models. Occasional positive reward spikes occurred when the agent made progress toward destinations, but these were typically followed by significant penalties from collisions or traffic violations, preventing the accumulation of stable learning signals.\n",
    "\n",
    "## Success and Collision Rates\n",
    "\n",
    "As illustrated in Figure 2, we did not achieve high success rates. \n",
    "\n",
    "![Success Rate](assets/success_rate.png)\n",
    "*Figure 2: Success rate over training episodes. The agent achieved successful navigation in less than 0.2% of episodes even after extensive training.*\n",
    "\n",
    "Additionally, the collision rate remained high, as shown in Figure 3.\n",
    "\n",
    "![Collision Rate](assets/collision_rate.png)\n",
    "*Figure 3: Average number of collisions per episode. Despite some reduction in collisions over time, the rate remained high (80-140 collisions per episode).*\n",
    "\n",
    "## Action Distribution Analysis\n",
    "\n",
    "To better understand the agent's behavior, we analyzed the distribution of actions selected during later training episodes. Figure 4 shows this distribution after 5,000 episodes.\n",
    "\n",
    "![Action Distribution](assets/action_distribution.png)\n",
    "*Figure 4: Distribution of actions selected by the agent. The policy shows a bias toward throttle and steering actions, with limited use of braking.*\n",
    "\n",
    "This distribution reveals that the agent developed a preference for forward motion (throttle) combined with steering, but rarely utilized braking effectively. This partially explains the high collision rate, as the agent failed to learn appropriate stopping behavior in response to obstacles.\n",
    "\n",
    "## Visual Processing Capabilities\n",
    "\n",
    "We examined the agent's visual processing by visualizing activation maps from the convolutional network layers using Gradient-weighted Class Activation Mapping (GradCAM). Figure 5 shows sample camera input alongside the corresponding attention visualization.\n",
    "\n",
    "\n",
    "![Visual Processing](assets/visual_attention.png)\n",
    "*Figure 5: Camera input (left) and corresponding network attention visualization (right). The attention maps suggest the network focuses on road boundaries but struggles with obstacle detection.*\n",
    "\n",
    "<INSERT RESULTS HERE>\n",
    "\n",
    "## Learning Curve Analysis\n",
    "\n",
    "Finally, we analyzed the agent's learning curve by plotting the average value estimates against actual returns, as shown in Figure 6.\n",
    "\n",
    "![Value Estimation](assets/value_estimation.png)\n",
    "*Figure 6: Comparison of predicted state values versus actual returns. The persistent gap indicates the agent's difficulty in accurately estimating state values.*\n",
    "\n",
    "<INSERT RESULTS HERE>\n",
    "\n",
    "In summary, our results demonstrate that while the A3C agent showed some learning capability, it failed to master the complex task of autonomous driving from raw pixels. The high-dimensional state space, delayed rewards, and complex dynamics of the driving environment posed significant challenges for end-to-end reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFLKE2ztblZC"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The current implementation shows limited success in achieving its goal, with training metrics indicating a very low success rate (0.0-0.2%) even after 6000 episodes. The agent consistently experiences a high number of collisions. There are several limitations. Relying solely on a front-facing RGB camera creates perceptual limitations including blind spots and lack of depth perception. Learning directly from raw pixel inputs presents challenges due to the extremely high-dimensional state space and difficulty in extracting relevant features without extensive training. The reward structure provides insufficient learning signals, with the fixed rollout length of 20 steps being too short for meaningful learning in complex driving scenarios.. Additionally, real-time decision-making requirements constrain model complexity, and the lack of prior knowledge or encoded driving rules forces the agent to learn safety-critical behaviors purely from trial and error, which is highly inefficient.\n",
    "\n",
    "### Future work\n",
    "Future work should focus on several key improvements to address the current limitations:\n",
    "\n",
    "1. **Curriculum Learning**: Implement a structured progression of driving scenarios with increasing complexity:\n",
    "   - Start with basic straight-road navigation without obstacles\n",
    "   - Gradually introduce simple turns and intersections without traffic\n",
    "   - Add stationary obstacles before moving vehicles\n",
    "   - Finally introduce complex urban environments with traffic and pedestrians\n",
    "\n",
    "2. **Imitation Learning Integration**:\n",
    "   - Collect expert demonstrations in the CARLA environment\n",
    "   - Pre-train the network using behavioral cloning to learn basic driving skills\n",
    "   - This hybrid approach could significantly accelerate learning and provide a better initialization point before pure RL exploration\n",
    "\n",
    "3. **Architecture Enhancements**:\n",
    "   - Implement recurrent layers (LSTM/GRU) to capture temporal dependencies in driving sequences\n",
    "   - Explore attention mechanisms to help the model focus on relevant parts of the visual input\n",
    "   - Incorporate auxiliary prediction tasks (depth estimation, semantic segmentation) as additional learning signals\n",
    "   - Experiment with larger network architectures with more capacity for feature extraction\n",
    "\n",
    "4. **Safety Constraints**:\n",
    "   - Implement constrained reinforcement learning to enforce safety requirements\n",
    "   - Integrate rule-based safety monitors that can override unsafe actions\n",
    "   - Explore safe exploration techniques that limit catastrophic failures during training\n",
    "\n",
    "### Ethics & Privacy\n",
    "While this project uses simulated data, deploying similar autonomous driving systems in the real world raises significant ethical concerns regarding safety, liability, and decision-making in unavoidable accident scenarios. The reward function design implicitly encodes value judgments about the relative importance of different driving behaviors (e.g., weighing progress toward destinations against safety considerations), which requires careful ethical scrutiny. If deployed, such systems would need transparent decision-making processes and clear accountability frameworks for when accidents occur. Additionally, real-world implementations would involve collecting massive amounts of camera data from public spaces, raising privacy concerns that would require appropriate anonymization techniques and data governance policies.\n",
    "\n",
    "### Conclusion\n",
    "This project demonstrates both the potential and significant challenges of applying end-to-end reinforcement learning to autonomous driving tasks. Despite implementing state-of-the-art techniques like Actor-Critic networks and prioritized experience replay, the results show that learning directly from pixels remains extremely difficult in complex, dynamic environments like CARLA. The poor performance metrics highlight the need for hybrid approaches that combine reinforcement learning with more structured representations, imitation learning, and explicit safety constraints. Future work should focus on decomposing the driving task into more manageable sub-problems and incorporating domain knowledge to guide learning, rather than relying solely on end-to-end optimization from raw sensory inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz2YyBH6blZC"
   },
   "source": [
    "# Footnotes\n",
    "[1] <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Hao Shao, Letian Wang, Ruobing Chen, Steven L. Waslander, Hongsheng Li, Yu Liu. (2023). ReasonNet: End-to-End Driving with Temporal and Global Reasoning.\n",
    "\n",
    "[2] <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, Yu Qiao. (2022). Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline.\n",
    "\n",
    "[3] <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Hongyan Guo, Dongpu C., Hong C., Zhenping S., Yungfeng H. (2019). Model predictive path following control for autonomous cars considering a measurable disturbance: Implementation, testing, and verification.\n",
    "\n",
    "\n",
    "[4] <a name=\"cite_note-4\"></a> [^](#cite_ref-4) Jinxin Lui, Yugong Luo, Zhihua Z., Kequiang Li, Heye H., Hui X. (2022). A Probabilistic Architecture of Long-Term Vehicle Trajectory Prediction for Autonomous Driving.\n",
    "\n",
    "[5] <a name=\"cite_note-5\"></a> [^](#cite_ref-5) Faisal, A., Kamruzzaman, M., Yigitcanlar, T., & Currie, G. (2019). Understanding autonomous vehicles: A systematic literature review on capability, impact, planning and policy. Journal of Transport and Land Use, 12(1), 45–72.\n",
    "\n",
    "[6] <a name=\"cite_note-6\"></a> [^](#cite_ref-6) Ros, G., Koltun, V., & Lopez, A. M. (2017). Carla simulator. Introduction - CARLA Simulator.\n",
    "\n",
    "[7]  <a name=\"cite_note-7\"></a> [^](#cite_ref-7) Shao, Hao, et al. (2022) Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Materials\n",
    "\n",
    "## Network Architecture\n",
    "The neural network follows a **Convolutional Actor-Critic** architecture with shared feature extraction layers and separate **Actor** and **Critic** heads.\n",
    "\n",
    "### Convolutional Layers:\n",
    "- **Conv1:** 3 → 16 channels, **5×5** kernel, **stride 2**\n",
    "- **MaxPool:** **2×2**\n",
    "- **Conv2:** 16 → 32 channels, **5×5** kernel, **stride 2**\n",
    "- **MaxPool:** **2×2**\n",
    "\n",
    "### Fully Connected Layers:\n",
    "- **FC Common:** 32×15×23 → **128 neurons**\n",
    "- **Actor Head:** 128 → **6 actions**\n",
    "- **Critic Head:** 128 → **1 value**\n",
    "\n",
    "## Experience Replay Buffer:\n",
    "- **Buffer Capacity:** 100,000 transitions.\n",
    "- **Batch Size:** 256 transitions per update.\n",
    "- **Sampling Strategy:** Higher priority given to experiences with greater TD errors for improved off-policy learning.\n",
    "\n",
    "## Reward Function Components\n",
    "\n",
    "### **Speed Compliance**\n",
    "- **+0.3** for maintaining safe speeds  \n",
    "- **-0.3** for exceeding speed limits  \n",
    "\n",
    "### **Collision Penalties**\n",
    "- **-5 to -40**, based on collision severity (e.g., pedestrian vs. static object)  \n",
    "\n",
    "### **Progress Reward**\n",
    "- **+1.5 × (previous_distance - current_distance)**  \n",
    "- Encourages continuous movement toward the goal  \n",
    "\n",
    "### **Destination Bonus**\n",
    "- **+2000** upon reaching the destination  \n",
    "\n",
    "### **Traffic Light Penalty**\n",
    "- **-5.0** for running a red light  \n",
    "\n",
    "### **Lane Keeping Penalty**\n",
    "- **-0.1 × (heading_difference - 15°)** for deviations from the lane  \n",
    "\n",
    "## Exploration Strategy\n",
    "The agent follows an **epsilon-greedy strategy** with gradual decay to balance exploration and exploitation:\n",
    "\n",
    "- **Initial ε = 1.0** (100% random actions)\n",
    "- **Minimum ε = 0.2** (20% random actions)\n",
    "- **Decay rate = 0.9999** per episode\n",
    "\n",
    "## Implementation Details\n",
    "The implementation is built using the following Python libraries:\n",
    "\n",
    "- **PyTorch**: Neural network training and optimization  \n",
    "- **CARLA**: Autonomous driving simulation  \n",
    "- **NumPy**: Numerical computations  \n",
    "- **OpenCV**: Image preprocessing and traffic light detection  \n",
    "\n",
    "This structured approach ensures the agent learns effective driving policies while adhering to safety constraints.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
